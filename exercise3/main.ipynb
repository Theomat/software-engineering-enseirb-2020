{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "import json\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the content of the json file as a dict.\n",
    "    \"\"\"\n",
    "    f = open(filename, 'r')\n",
    "    file_content = json.load(f)\n",
    "    f.close()\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def get_raw_training_data(path: str = './training_set.json'):\n",
    "    \"\"\"\n",
    "    Load the raw training set.\n",
    "    \"\"\"\n",
    "    return load_json_data(path)\n",
    "\n",
    "\n",
    "def get_raw_testing_data(path: str = './testing_set.json'):\n",
    "    \"\"\"\n",
    "    Load the raw testing set.\n",
    "    \"\"\"\n",
    "    return load_json_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data = get_raw_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [v['sentence'] for v in raw_training_data]\n",
    "y = [v['intent'] for v in raw_training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "logging.getLogger('spacy_lefff').setLevel(logging.WARNING)\n",
    "\n",
    "pos = POSTagger()\n",
    "french_lemmatizer = LefffLemmatizer(after_melt=True)\n",
    "nlp.add_pipe(pos, name='pos', after='parser')\n",
    "nlp.add_pipe(french_lemmatizer, name='lefff', after='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(nlp, X):\n",
    "    new_X = []\n",
    "    for sentence in X:\n",
    "        doc = nlp(sentence)\n",
    "        lemmas = []\n",
    "        for token in doc:                                # Tokenize\n",
    "            if not token.is_stop and not token.is_punct: # Remove stop words and punctuation\n",
    "                lemmas.append(token)                     # Because of pipes added, each token has it's lemma\n",
    "        new_X.append(lemmas)\n",
    "    return new_X\n",
    "\n",
    "X = pre_process(nlp, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pouvez pouvoir\n",
      "réserver réserver\n",
      "09h 09h\n",
      "oui oui\n"
     ]
    }
   ],
   "source": [
    "for token in X[6]:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
